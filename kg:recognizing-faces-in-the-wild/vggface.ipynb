{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/rcmalli/keras-vggface.git\n",
      "  Cloning https://github.com/rcmalli/keras-vggface.git to /tmp/pip-req-build-k0xf2rgl\n",
      "  Running command git clone -q https://github.com/rcmalli/keras-vggface.git /tmp/pip-req-build-k0xf2rgl\n",
      "Requirement already satisfied (use --upgrade to upgrade): keras-vggface==0.5 from git+https://github.com/rcmalli/keras-vggface.git in /home/stps/.virtualenvs/kaggle3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/stps/.virtualenvs/kaggle3/lib/python3.6/site-packages (from keras-vggface==0.5) (1.16.3)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/stps/.virtualenvs/kaggle3/lib/python3.6/site-packages (from keras-vggface==0.5) (1.3.0)\n",
      "Requirement already satisfied: h5py in /home/stps/.virtualenvs/kaggle3/lib/python3.6/site-packages (from keras-vggface==0.5) (2.9.0)\n",
      "Requirement already satisfied: pillow in /home/stps/.virtualenvs/kaggle3/lib/python3.6/site-packages (from keras-vggface==0.5) (6.0.0)\n",
      "Requirement already satisfied: keras in /home/stps/.virtualenvs/kaggle3/lib/python3.6/site-packages (from keras-vggface==0.5) (2.2.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/stps/.virtualenvs/kaggle3/lib/python3.6/site-packages (from keras-vggface==0.5) (1.12.0)\n",
      "Requirement already satisfied: pyyaml in /home/stps/.virtualenvs/kaggle3/lib/python3.6/site-packages (from keras-vggface==0.5) (5.1)\n",
      "Collecting keras-applications==1.0.2 (from keras->keras-vggface==0.5)\n",
      "  Using cached https://files.pythonhosted.org/packages/e2/60/c557075e586e968d7a9c314aa38c236b37cb3ee6b37e8d57152b1a5e0b47/Keras_Applications-1.0.2-py2.py3-none-any.whl\n",
      "Collecting keras-preprocessing==1.0.1 (from keras->keras-vggface==0.5)\n",
      "  Using cached https://files.pythonhosted.org/packages/f8/33/275506afe1d96b221f66f95adba94d1b73f6b6087cfb6132a5655b6fe338/Keras_Preprocessing-1.0.1-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: keras-vggface\n",
      "  Building wheel for keras-vggface (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-gl_gqkc9/wheels/36/07/46/06c25ce8e9cd396dabe151ea1d8a2bc28dafcb11321c1f3a6d\n",
      "Successfully built keras-vggface\n",
      "\u001b[31mERROR: tensorflow 1.13.1 has requirement keras-applications>=1.0.6, but you'll have keras-applications 1.0.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow 1.13.1 has requirement keras-preprocessing>=1.0.5, but you'll have keras-preprocessing 1.0.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: keras-applications, keras-preprocessing\n",
      "  Found existing installation: Keras-Applications 1.0.7\n",
      "    Uninstalling Keras-Applications-1.0.7:\n",
      "      Successfully uninstalled Keras-Applications-1.0.7\n",
      "  Found existing installation: Keras-Preprocessing 1.0.9\n",
      "    Uninstalling Keras-Preprocessing-1.0.9:\n",
      "      Successfully uninstalled Keras-Preprocessing-1.0.9\n",
      "Successfully installed keras-applications-1.0.2 keras-preprocessing-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/rcmalli/keras-vggface.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from glob import glob\n",
    "from os import path\n",
    "from typing import Dict, Tuple, List, Set, Callable, Iterator\n",
    "\n",
    "import attr\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas\n",
    "from keras import Input, Model\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.engine import InputLayer\n",
    "from keras.layers import Concatenate, GlobalMaxPool2D, Subtract, Multiply, \\\n",
    "    Dense, Dropout, GlobalAvgPool2D\n",
    "from keras.optimizers import Adam\n",
    "from keras_vggface import VGGFace\n",
    "\n",
    "PersonId = Tuple[str, str]\n",
    "TrainValDataType = Iterator[Tuple[List[np.ndarray], List[int]]]\n",
    "TestDataType = Tuple[List[np.ndarray], List[np.ndarray], List[str]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_dir_path(relative_path: str) -> str:\n",
    "    ret = path.abspath(path.expanduser(relative_path))\n",
    "    if not ret.endswith('/'):\n",
    "        ret = ret + '/'\n",
    "    return ret\n",
    "\n",
    "\n",
    "@attr.s\n",
    "class Params(object):\n",
    "    # Test mode runs quickly and produces horrible results!\n",
    "    test_mode: float = attr.attrib(default=False)\n",
    "\n",
    "    families_val_split: float = attr.attrib(default=0.1)\n",
    "    input_data_dir: str = attr.attrib(default=full_dir_path('../input/'))\n",
    "    output_data_dir: str = attr.attrib(default=full_dir_path('./'))\n",
    "    # 1 +ve sample, 1 -ve sample.\n",
    "    negative_sample_ratio: float = attr.attrib(default=0.5)\n",
    "    # Batches of 32 (16 +ve, 16 -ve)\n",
    "    batch_size: int = attr.attrib(default=32)\n",
    "    # update_params overrides this in test_mode\n",
    "    epochs: int = attr.attrib(default=100)\n",
    "    # update_params overrides this in test_mode\n",
    "    steps_per_epoch: int = attr.attrib(default=200)\n",
    "    # update_params overrides this in test_mode\n",
    "    validation_steps: int = attr.attrib(default=100)\n",
    "\n",
    "    def get_inp_path(self, file_name):\n",
    "        return self.input_data_dir + file_name\n",
    "\n",
    "    def get_out_path(self, file_name):\n",
    "        return self.output_data_dir + file_name\n",
    "\n",
    "    def update_params(self, config_file_path):\n",
    "        try:\n",
    "            with open(config_file_path, \"r\") as f:\n",
    "                config = json.load(f)\n",
    "                print(\"Before update_params:\", self)\n",
    "                for k, v in config.items():\n",
    "                    if k.endswith('_dir'):\n",
    "                        v = full_dir_path(v)\n",
    "                    self.__dict__[k] = v\n",
    "            print(\"After update_params:\", self)\n",
    "        except Exception as e:\n",
    "            print(\"Exception while loading config.json. This is okay when \"\n",
    "                  \"running on kaggle. Error:\", e)\n",
    "        if self.test_mode:\n",
    "            self.epochs = min(self.epochs, 5)\n",
    "            self.validation_steps = min(self.validation_steps, 5)\n",
    "            self.steps_per_epoch = min(self.steps_per_epoch, 2)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    @staticmethod\n",
    "    def _load_image_as_array(filepath: str) -> np.ndarray:\n",
    "        with open(filepath, 'rb') as f:\n",
    "            # noinspection PyTypeChecker\n",
    "            nparr = np.fromstring(f.read(), np.uint8)\n",
    "            # noinspection PyUnresolvedReferences\n",
    "            return cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_train_images(params: Params) -> Dict[PersonId, List[np.ndarray]]:\n",
    "        ret: Dict[PersonId, List[np.ndarray]] = {}\n",
    "        all_images = glob(params.get_inp_path('train/') + \"*/*/*.jpg\")\n",
    "        for name in all_images:\n",
    "            img_np = Data._load_image_as_array(name)\n",
    "            p: PersonId = (name.split('/')[-3], name.split('/')[-2])\n",
    "            if p not in ret:\n",
    "                ret[p] = []\n",
    "            ret[p].append(img_np)\n",
    "            if params.test_mode and len(ret) > 200:\n",
    "                break\n",
    "        print(\"Train+Val images: \", sum(len(x) for x in ret.values()))\n",
    "        print(\"Train+Val people: \", len(ret))\n",
    "        return ret\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_test_images(params: Params) -> Dict[str, np.ndarray]:\n",
    "        ret: Dict[str, np.ndarray] = {}\n",
    "        all_images = glob(params.get_inp_path('test/') + \"*.jpg\")\n",
    "        for name in all_images:\n",
    "            img_np = Data._load_image_as_array(name)\n",
    "            ret[name.split('/')[-1]] = img_np\n",
    "            if params.test_mode and len(ret) > 500:\n",
    "                break\n",
    "        print(\"Test images: \", len(ret))\n",
    "        return ret\n",
    "\n",
    "    @staticmethod\n",
    "    def _train_or_val_batch_generator(\n",
    "            params: Params, images: Dict[PersonId, List[np.ndarray]],\n",
    "            relations: List[Tuple[PersonId, PersonId]],\n",
    "            people: List[PersonId]) -> \\\n",
    "            Iterator[TrainValDataType]:\n",
    "        negatives = int(\n",
    "            params.batch_size * params.negative_sample_ratio)\n",
    "        positives = params.batch_size - negatives\n",
    "        while True:\n",
    "            x1: List[np.ndarray] = []\n",
    "            x2: List[np.ndarray] = []\n",
    "            out: List[int] = []\n",
    "            while len(x1) < positives:\n",
    "                p1, p2 = random.choice(relations)\n",
    "                if p1 not in images or p2 not in images:\n",
    "                    continue\n",
    "                p1img, p2img = random.choice(images[p1]), \\\n",
    "                               random.choice(images[p2])\n",
    "                x1.append(p1img)\n",
    "                x2.append(p2img)\n",
    "                out.append(1)\n",
    "            while len(x1) < positives + negatives:\n",
    "                p1 = random.choice(people)\n",
    "                p2 = random.choice(people)\n",
    "                if (p1, p2) in relations or (p2, p1) in relations:\n",
    "                    continue\n",
    "                p1img, p2img = random.choice(images[p1]), \\\n",
    "                               random.choice(images[p2])\n",
    "                x1.append(p1img)\n",
    "                x2.append(p2img)\n",
    "                out.append(0)\n",
    "            yield [np.array(x1), np.array(x2)], out\n",
    "\n",
    "    def __init__(self, params: Params):\n",
    "        self.params = params\n",
    "        self.images: Dict[\n",
    "            PersonId, List[np.ndarray]] = Data._load_train_images(self.params)\n",
    "        self.test_images: Dict[str, np.ndarray] = Data._load_test_images(\n",
    "            self.params)\n",
    "\n",
    "        # Generate validation relations and train relations.\n",
    "        # noinspection PyTypeChecker\n",
    "        self.val_relations: List[Tuple[PersonId, PersonId]] = []\n",
    "        # noinspection PyTypeChecker\n",
    "        self.train_relations: List[Tuple[PersonId, PersonId]] = []\n",
    "        get_person_id: Callable[[str], PersonId] = \\\n",
    "            lambda x: (x.split('/')[0], x.split('/')[1])\n",
    "        relations_df: pandas.DataFrame = pandas.read_csv(\n",
    "            self.params.get_inp_path('train_relationships.csv'))\n",
    "        relations: List[Tuple[PersonId, PersonId]] = \\\n",
    "            list(zip(list(relations_df[\"p1\"].apply(get_person_id)),\n",
    "                     list(relations_df[\"p1\"].apply(get_person_id))))\n",
    "        if not params.test_mode:\n",
    "            random.shuffle(relations)\n",
    "        _families = set(x[0][0] for x in relations)\n",
    "        _families.update(set(x[1][0] for x in relations))\n",
    "        _families = list(_families)\n",
    "        val_families: Set[str] = set(_families[:int(\n",
    "            len(_families) * self.params.families_val_split)])\n",
    "        for relation in relations:\n",
    "            if relation[0][0] in val_families or relation[1][0] in val_families:\n",
    "                self.val_relations.append(relation)\n",
    "            else:\n",
    "                self.train_relations.append(relation)\n",
    "        # Split people into train and validation.\n",
    "        self.train_people: List[PersonId] = []\n",
    "        self.val_people: List[PersonId] = []\n",
    "        for person in self.images.keys():\n",
    "            if person[0] in val_families:\n",
    "                self.val_people.append(person)\n",
    "            else:\n",
    "                self.train_people.append(person)\n",
    "\n",
    "        # Generate test data.\n",
    "        # [img_par], [p1_image], [p2_image]\n",
    "        self.test_data: TestDataType = ([], [], [])\n",
    "        test_img_pairs = pandas.read_csv(\n",
    "            self.params.get_inp_path('sample_submission.csv'))['img_pair']\n",
    "        failed_image_pairs = []\n",
    "        for img_pair in test_img_pairs:\n",
    "            img1, img2 = img_pair.split('-')\n",
    "            img1, img2 = self.test_images.get(img1), self.test_images.get(img2)\n",
    "            if img1 is not None and img2 is not None:\n",
    "                self.test_data[0].append(img1)\n",
    "                self.test_data[1].append(img2)\n",
    "                self.test_data[2].append(img_pair)\n",
    "            else:\n",
    "                failed_image_pairs.append(img_pair)\n",
    "        if failed_image_pairs:\n",
    "            print(\"Failed to find {} image pairs\".format(\n",
    "                len(failed_image_pairs)))\n",
    "            print(\"pairs: \", ','.join(failed_image_pairs)[:500], \" ...\")\n",
    "\n",
    "    def get_test_data(self) -> \\\n",
    "            Tuple[List[np.ndarray], List[np.ndarray], List[str]]:\n",
    "        return self.test_data\n",
    "\n",
    "    def val_batch_generator(self) -> Iterator[TrainValDataType]:\n",
    "        return Data._train_or_val_batch_generator(self.params, self.images,\n",
    "                                                  self.val_relations,\n",
    "                                                  self.val_people)\n",
    "\n",
    "    def train_batch_generator(self) -> Iterator[TrainValDataType]:\n",
    "        return Data._train_or_val_batch_generator(self.params, self.images,\n",
    "                                                  self.train_relations,\n",
    "                                                  self.train_people)\n",
    "\n",
    "    def print_summary(self):\n",
    "        print('Test data info: i1-shape: {} i2-shape: {}'.format(\n",
    "            np.stack(self.test_data[0]).shape,\n",
    "            np.stack(self.test_data[1]).shape))\n",
    "        for X, y in self.train_batch_generator():\n",
    "            print('Train batch shapes: x1: {} x2: {} y: {}'.format(\n",
    "                X[0].shape, X[1].shape, len(y)))\n",
    "            break\n",
    "        for X, y in self.val_batch_generator():\n",
    "            print('Val batch shapes: x1: {} x2: {} y: {}'.format(\n",
    "                X[0].shape, X[1].shape, len(y)))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def create_model() -> Model:\n",
    "    input1: InputLayer = Input(shape=(224, 224, 3))\n",
    "    input2: InputLayer = Input(shape=(224, 224, 3))\n",
    "\n",
    "    base_model: Model = VGGFace(model='resnet50', include_top=False)\n",
    "\n",
    "    # Make last 3 layers trainable.\n",
    "    for x in base_model.layers[:-3]:\n",
    "        x.trainable = True\n",
    "\n",
    "    # Transform image1\n",
    "    x1 = base_model(input1)\n",
    "    x1 = Concatenate()([GlobalMaxPool2D()(x1), GlobalAvgPool2D()(x1)])\n",
    "\n",
    "    # Transform image2\n",
    "    x2 = base_model(input2)\n",
    "    x2 = Concatenate()([GlobalMaxPool2D()(x2), GlobalAvgPool2D()(x2)])\n",
    "\n",
    "    _diff = Subtract()([x1, x2])\n",
    "    diff_squared = Multiply()([_diff, _diff])\n",
    "\n",
    "    # concat(x1.x2, (x1-x2)**2)\n",
    "    x = Concatenate()([Multiply()([x1, x2]), diff_squared])\n",
    "    x = Dense(100, activation=\"relu\")(x)\n",
    "    # TODO(dotslash): Not sure about the dropout prob.\n",
    "    x = Dropout(0.2)(x)\n",
    "    out = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model([input1, input2], out)\n",
    "    model.compile(loss=\"binary_crossentropy\", metrics=['acc'],\n",
    "                  optimizer=Adam(0.00001))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception while loading config.json. This is okay when running on kaggle. Error: [Errno 2] No such file or directory: './config.json'\n"
     ]
    }
   ],
   "source": [
    "params: Params = Params().update_params('./config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train+Val images:  12379\n",
      "Train+Val people:  2316\n",
      "Test images:  6282\n",
      "Test data info: i1-shape: (5310, 224, 224, 3) i2-shape: (5310, 224, 224, 3)\n",
      "Train batch shapes: x1: (32, 224, 224, 3) x2: (32, 224, 224, 3) y: 32\n",
      "Val batch shapes: x1: (32, 224, 224, 3) x2: (32, 224, 224, 3) y: 32\n"
     ]
    }
   ],
   "source": [
    "data: Data = Data(params)\n",
    "data.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/rcmalli/keras-vggface/releases/download/v2.0/rcmalli_vggface_tf_notop_resnet50.h5\n",
      "94699520/94694792 [==============================] - 1s 0us/step\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "vggface_resnet50 (Model)        multiple             23561152    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_1 (GlobalM (None, 2048)         0           vggface_resnet50[1][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 2048)         0           vggface_resnet50[1][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_2 (GlobalM (None, 2048)         0           vggface_resnet50[2][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glo (None, 2048)         0           vggface_resnet50[2][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 4096)         0           global_max_pooling2d_1[0][0]     \n",
      "                                                                 global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 4096)         0           global_max_pooling2d_2[0][0]     \n",
      "                                                                 global_average_pooling2d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "subtract_1 (Subtract)           (None, 4096)         0           concatenate_1[0][0]              \n",
      "                                                                 concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 4096)         0           concatenate_1[0][0]              \n",
      "                                                                 concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 4096)         0           subtract_1[0][0]                 \n",
      "                                                                 subtract_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 8192)         0           multiply_2[0][0]                 \n",
      "                                                                 multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          819300      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 100)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            101         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 24,380,553\n",
      "Trainable params: 24,327,433\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " 35/200 [====>.........................] - ETA: 2:06 - loss: 2.7163 - acc: 0.6875"
     ]
    }
   ],
   "source": [
    "save_best_model = ModelCheckpoint(params.get_out_path('best_model.h5'),\n",
    "                                  monitor='val_acc', verbose=1,\n",
    "                                  save_best_only=True)\n",
    "control_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10)\n",
    "stop_early = EarlyStopping(monitor='val_loss', patience=20)\n",
    "model.fit_generator(generator=data.train_batch_generator(),\n",
    "                    validation_data=data.val_batch_generator(),\n",
    "                    steps_per_epoch=params.steps_per_epoch,\n",
    "                    validation_steps=params.validation_steps,\n",
    "                    epochs=params.epochs,\n",
    "                    callbacks=[save_best_model, control_lr, stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_on_batch(\n",
    "    x=[data.test_data[0], data.test_data[1]])\n",
    "submission = pandas.DataFrame(\n",
    "    data={'is_related': predictions.flatten(),\n",
    "          'img_pair': data.test_data[2]})\n",
    "submission.to_csv(params.get_out_path(\"output.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
