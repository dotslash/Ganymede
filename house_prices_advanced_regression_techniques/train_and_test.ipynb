{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformtion: Follow the findings of data_exploration.ipynb\n",
    "def DropCol(df, col):\n",
    "    if col in df.columns: df.drop([col], axis=1, inplace=True)\n",
    "\n",
    "def HandleEmptyValues(all_data):\n",
    "    all_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")\n",
    "    all_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")\n",
    "    all_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")\n",
    "    all_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\n",
    "    all_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")\n",
    "    all_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n",
    "        lambda x: x.fillna(x.median()))\n",
    "    for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n",
    "        all_data[col] = all_data[col].fillna('None')\n",
    "    for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n",
    "        all_data[col] = all_data[col].fillna(0)\n",
    "    for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n",
    "        all_data[col] = all_data[col].fillna(0)\n",
    "    for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n",
    "        all_data[col] = all_data[col].fillna('None')\n",
    "    all_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\n",
    "    all_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\n",
    "    all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\n",
    "    all_data = all_data.drop(['Utilities'], axis=1)\n",
    "    all_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\n",
    "    all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\n",
    "    all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\n",
    "    all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\n",
    "    all_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\n",
    "    all_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\n",
    "    all_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")\n",
    "    return all_data\n",
    "\n",
    "\n",
    "from scipy.stats import norm, skew #for some statistics\n",
    "def BeatSkew(all_data):\n",
    "    numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "    # Check the skew of all numerical features\n",
    "    skewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "    print(\"\\nSkew in numerical features: \\n\")\n",
    "    skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "    # skewness.head(10)\n",
    "    skewness = skewness[abs(skewness) > 0.75]\n",
    "    print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n",
    "\n",
    "    from scipy.special import boxcox1p\n",
    "    skewed_features = skewness.index\n",
    "    lam = 0.15\n",
    "    for feat in skewed_features:\n",
    "        if feat in {'SalePrice'} : continue\n",
    "        all_data[feat] = boxcox1p(all_data[feat], lam)\n",
    "    return all_data\n",
    "\n",
    "def Transform(df):\n",
    "    df = HandleEmptyValues(df)\n",
    "    \n",
    "    # MSSubClass=The building class\n",
    "    df['MSSubClass'] = df['MSSubClass'].apply(str)\n",
    "    # Changing OverallCond into a categorical variable\n",
    "    df['OverallCond'] = df['OverallCond'].astype(str)\n",
    "    # Year and month sold are transformed into categorical features.\n",
    "    # df['YrSold'] = df['YrSold'].astype(str)\n",
    "    df['MoSold'] = df['MoSold'].astype(str)\n",
    "\n",
    "    cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n",
    "        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n",
    "        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n",
    "        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n",
    "         # 'YrSold', \n",
    "        'MoSold')\n",
    "    # process columns, apply LabelEncoder to categorical features\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    for c in cols:\n",
    "        lbl = LabelEncoder() \n",
    "        lbl.fit(list(all_data[c].values)) \n",
    "        all_data[c] = lbl.transform(list(all_data[c].values))\n",
    "    \n",
    "\n",
    "    df['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n",
    "\n",
    "    # Normalize Data.\n",
    "    # create column for new variable (one is enough because it's a binary categorical feature)\n",
    "    # if area>0 it gets 1, for area==0 it gets 0\n",
    "    df['HasBsmt'] = pd.Series(len(df['TotalBsmtSF']), index=df.index)\n",
    "    df['HasBsmt'] = 0 \n",
    "    df.loc[df['TotalBsmtSF'] > 0, 'HasBsmt'] = 1\n",
    "    # TotalBsmtSF -> log(TotalBsmtSF)\n",
    "    # df.loc[df['HasBsmt']==1,'TotalBsmtSF'] = np.log(df['TotalBsmtSF'])    \n",
    "    # Each value for a categorical field is a new new field.\n",
    "    df = BeatSkew(df)\n",
    "    return pd.get_dummies(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Skew in numerical features: \n",
      "\n",
      "There are 36 skewed numerical features to Box Cox transform\n",
      "set()\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test.csv')\n",
    "\n",
    "\n",
    "test_ID = df_test['Id']\n",
    "\n",
    "all_data = pd.concat([df_train, df_test]).reset_index(drop=True)\n",
    "DropCol(all_data, \"Id\")\n",
    "\n",
    "all_data_trans = Transform(all_data)\n",
    "train, test = all_data_trans[:len(df_train)], all_data_trans[len(df_train):]\n",
    "\n",
    "train = train.drop(train[(train['GrLivArea'] > 4000) & (train['SalePrice'] < 300000)].index)\n",
    "\n",
    "y_train = np.log1p(train.SalePrice.values)\n",
    "DropCol(train, \"SalePrice\")\n",
    "DropCol(test, \"SalePrice\")\n",
    "\n",
    "\n",
    "print(set(train.columns).difference(set(test.columns)))\n",
    "print(set(test.columns).difference(set(train.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard\n",
    "# We use the cross_val_score function of Sklearn. \n",
    "# However this function has not a shuffle attribut, \n",
    "# we add then one line of code, in order to shuffle the\n",
    "# dataset prior to cross-validation\n",
    "def rmsle_cv(model):\n",
    "    kf = KFold(5, shuffle=True, random_state=42).get_n_splits(train.values)\n",
    "    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)\n",
    "\n",
    "# This model may be very sensitive to outliers. So we need to made it more\n",
    "# robust on them. For that we use the sklearn's Robustscaler() method on pipeline\n",
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n",
    "\n",
    "# Elastic Net Regression : again made robust to outliers\n",
    "ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n",
    "\n",
    "# Kernel Ridge Regression\n",
    "KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "\n",
    "# Gradient Boosting Regression : With huber loss that makes it robust to outliers\n",
    "GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)\n",
    "# XGBoost\n",
    "model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                             learning_rate=0.05, max_depth=3, \n",
    "                             min_child_weight=1.7817, n_estimators=2200,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.5213, silent=1,\n",
    "                             random_state =7, nthread = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso score: 0.1238 (0.0164)\n",
      "ElasticNet score: 0.1239 (0.0163)\n",
      "Kernel Ridge score: 0.1265 (0.0126)\n",
      "Gradient Boosting score: 0.1256 (0.0118)\n",
      "Xgboost score: 0.1233 (0.0099)\n"
     ]
    }
   ],
   "source": [
    "score = rmsle_cv(lasso)\n",
    "print(\"Lasso score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
    "\n",
    "score = rmsle_cv(ENet)\n",
    "print(\"ElasticNet score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
    "\n",
    "score = rmsle_cv(KRR)\n",
    "print(\"Kernel Ridge score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
    "\n",
    "score = rmsle_cv(GBoost)\n",
    "print(\"Gradient Boosting score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
    "\n",
    "score = rmsle_cv(model_xgb)\n",
    "print(\"Xgboost score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    # we define clones of the original models to fit the data in\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "        \n",
    "        # Train cloned base models\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    #Now we do the predictions for cloned models and average them\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([\n",
    "            model.predict(X) for model in self.models_\n",
    "        ])\n",
    "        return np.mean(predictions, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Averaged base models score: 0.1204 (0.0143)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n",
    "score = rmsle_cv(averaged_models)\n",
    "print(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds=5):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "   \n",
    "    # We again fit the data on clones of the original models\n",
    "    def fit(self, X, y):\n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
    "        \n",
    "        # Train cloned base models then create out-of-fold predictions\n",
    "        # that are needed to train the cloned meta-model\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                instance.fit(X[train_index], y[train_index])\n",
    "                y_pred = instance.predict(X[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "                \n",
    "        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "   \n",
    "    # Do the predictions of all base models on the test data and use the averaged predictions as \n",
    "    # meta-features for the final prediction which is done by the meta-model\n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        return self.meta_model_.predict(meta_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Averaged models score: 0.1211 (0.0143)\n"
     ]
    }
   ],
   "source": [
    "stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n",
    "                                                 meta_model = lasso)\n",
    "\n",
    "score = rmsle_cv(stacked_averaged_models)\n",
    "print(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.4603, gamma=0.0468, learning_rate=0.05,\n",
       "       max_delta_step=0, max_depth=3, min_child_weight=1.7817,\n",
       "       missing=None, n_estimators=2200, n_jobs=1, nthread=-1,\n",
       "       objective='reg:linear', random_state=7, reg_alpha=0.464,\n",
       "       reg_lambda=0.8571, scale_pos_weight=1, seed=None, silent=1,\n",
       "       subsample=0.5213)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rmsle(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "stacked_averaged_models.fit(train.values, y_train)\n",
    "model_xgb.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0849998068434901\n",
      "0.07954437546507045\n"
     ]
    }
   ],
   "source": [
    "stacked_train_pred = stacked_averaged_models.predict(train.values)\n",
    "stacked_pred = np.expm1(stacked_averaged_models.predict(test.values))\n",
    "print(rmsle(y_train, stacked_train_pred))\n",
    "\n",
    "\n",
    "xgb_train_pred = model_xgb.predict(train)\n",
    "xgb_pred = np.expm1(model_xgb.predict(test))\n",
    "print(rmsle(y_train, xgb_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = stacked_pred*0.80 + xgb_pred*0.20\n",
    "sub = pd.DataFrame()\n",
    "\n",
    "sub['Id'] = test_ID\n",
    "sub['SalePrice'] = stacked_pred\n",
    "sub.to_csv('submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
